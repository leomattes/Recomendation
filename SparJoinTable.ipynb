{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join large date sets using Spark \n",
    "\n",
    "This notebook is a part of the project Library recomender system. To generet the datases used in this notebook, we have used spiders to colect biblographis registers and users trasactions over  many  library sites.\n",
    "The bibliograthic registers have been already unificated and associated with a unique identifier class \"codObra\";\n",
    "Fig 1 shows this process \n",
    "\n",
    "\n",
    "\n",
    "Ever time that the spider find new trasanctios, its necessary to convert the local id to the \"codObra\" id. This notebook is about to join tow dataset and create other as result.\n",
    "The first dataset to be use \"reg.txt  corelate  for each local id to codObra,  second data set containd new transactios with locals ids. \n",
    "The results is a new dataser with new transactios with one with the coraleted \"codObra\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark joinf table \") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.memory.fraction\", 0.8) \\\n",
    "    .config(\"spark.executor.memory\", \"14g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\" , \"8000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+\n",
      "|codtit|codusu|fonte|    titulo|\n",
      "+------+------+-----+----------+\n",
      "|     4|292421|Skoob|Fernão_Cap|\n",
      "|     4|270521|Skoob|Fernão_Cap|\n",
      "+------+------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path= '/home/JobResults/arrumatransacional/trsOld.csv'\n",
    "path= '/home/JobResults/arrumatransacional/trs.csv'\n",
    "transacional = spark.read.load(path, format=\"csv\", header=\"true\", sep=',')\n",
    "\n",
    "\n",
    "                    \n",
    "# Displays the content of the DataFrame to stdout\n",
    "transacional.registerTempTable(\"transacional\")\n",
    "transacional.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     fonte|count(1)|\n",
      "+----------+--------+\n",
      "|    IDG-RJ| 1486913|\n",
      "|Amorim_Lim|   57201|\n",
      "|     Skoob|19569713|\n",
      "|    Apoesp|    7527|\n",
      "|       smc| 7536946|\n",
      "|Colorado_U|   81136|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "sql = \"SELECt fonte, count(*) from  transacional group by fonte  \"\n",
    "#sql = \"SELECt lower(fonte) from transacional where codtit = 1735  \"\n",
    "\n",
    "result =  sqlContext.sql(sql)\n",
    "result.show(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|28739436|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result =  sqlContext.sql(\"SELECt count(*) from  transacional   \")\n",
    "\n",
    "result.show(5) # 38 (28) M !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "|_c0|  _c1|    _c2|\n",
      "+---+-----+-------+\n",
      "|101| 4591|apeoesp|\n",
      "|101|11266|    SMC|\n",
      "+---+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "path= '/home/JobResults/arrumatransacional/reg.cvs'\n",
    "\n",
    "registros = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"mode\", \"DROPMALFORMED\").load(path)\n",
    "                    \n",
    "# Displays the content of the DataFrame to stdout\n",
    "registros.registerTempTable(\"registros\")\n",
    "registros.show(2)\n",
    "\n",
    "# _c0 codObra\n",
    "# _c1 codtit \n",
    "# _c2 fonte  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT   *     from  registros  where _c0 = 1735  and _c2 like 'A%'\n",
      "+----+----+-------------+\n",
      "| _c0| _c1|          _c2|\n",
      "+----+----+-------------+\n",
      "|1735|1407|Amigos do Bem|\n",
      "|1735|1544|Amigos do Bem|\n",
      "+----+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sql =\"SELECT   distinct _c2     from  registros \"\n",
    "sql =\"SELECT   *     from  registros  where _c0 = 1735  and _c2 like 'A%'\"\n",
    "\n",
    "print(sql)\n",
    "result =  sqlContext.sql(sql)\n",
    "result.show(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT transacional.codusu , _c0 as codobra , titulo, transacional.fonte, 5 as rating   FROM transacional  INNER JOIN  registros  ON  concat(string( registros._c1), lower(registros._c2))  =   concat(string( transacional.codtit), lower(transacional.fonte)) and transacional.fonte = 'Apoesp' \n",
      "+------+-------+------+-----+------+\n",
      "|codusu|codobra|titulo|fonte|rating|\n",
      "+------+-------+------+-----+------+\n",
      "+------+-------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sql = ' SELECT transacional.codusu , _c0 as codobra , titulo, transacional.fonte, 5 as rating '\n",
    "sql  = sql  + '  FROM transacional '\n",
    "sql  = sql  +  ' INNER JOIN  registros  ON  concat(string( registros._c1), lower(registros._c2))  =  '\n",
    "sql  = sql  +  \" concat(string( transacional.codtit), lower(transacional.fonte))  \"\n",
    "\n",
    "\n",
    "print(sql)\n",
    "\n",
    "result =  sqlContext.sql(sql)\n",
    "result.show(3)\n",
    "\n",
    "result.registerTempTable(\"transacoesObras\")\n",
    "result.toPandas().to_csv('transacoesObras.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|16500598|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transactiosR =  sqlContext.sql(\"select count(*) from  transacoesObras \" )\n",
    "transactiosR.show(4) # 16M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+------+\n",
      "|codusu|codobra|titulo|fonte|rating|\n",
      "+------+-------+------+-----+------+\n",
      "+------+-------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "transactiosR =  sqlContext.sql(\"select * from  transacoesObras \")\n",
    "transactiosR.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
