{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word2Vec is a popular algorithm used for generating dense vector representations of words in large corpora using unsupervised learning. The resulting vectors have been shown to capture semantic relationships between the corresponding words and are used extensively for many downstream natural language processing (NLP) tasks like sentiment analysis, named entity recognition and machine translation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker BlazingText which provides efficient implementations of Word2Vec on\n",
    "\n",
    "- single CPU instance\n",
    "- single instance with multiple GPUs - P2 or P3 instances\n",
    "- multiple CPU instances (Distributed training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::587023234711:role/service-role/AmazonSageMaker-ExecutionRole-20190603T102730\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3 # amazon sdk library\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set s3 propreties\n",
    "S3 is needed to store and access the modelo inpout and out put \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://mattmahoney.net/dc/text8.zip -O text8.gz\n",
    "# Uncompressing\n",
    "#!gzip -d text8.gz -f\n",
    "# s3_input  -> trainf data\n",
    "#s3_output_location result date \n",
    "\n",
    "#!more transacoesObrasbag.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recomenderteste\n",
      "s3://recomenderteste/bookrecomender/output\n",
      "bookrecomender\n"
     ]
    }
   ],
   "source": [
    "bucket = 'recomenderteste' \n",
    "prefix = 'bookrecomender' #Replace with the prefix under which you want to store the data if needed\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://recomenderteste/bookrecomender/train\n"
     ]
    }
   ],
   "source": [
    "train_channel = prefix + '/train'\n",
    "sess.upload_data(path='transacoesObrasbag.csv', bucket=bucket, key_prefix=train_channel)\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "print(s3_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a sageMaker.estimator.Estimator object. This estimator will launch the training job.\n",
    "-- region_name \n",
    "-- container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "print(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.2xlarge',\n",
    "                                         train_volume_size = 5,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"batch_skipgram\",\n",
    "                             epochs=5,\n",
    "                             min_count=5,\n",
    "                             sampling_threshold=0.0001,\n",
    "                             learning_rate=0.05,\n",
    "                             window_size=5,\n",
    "                             vector_dim=100,\n",
    "                             negative_samples=5,\n",
    "                             batch_size=11, #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "                             evaluation=True,# Perform similarity evaluation on WS-353 dataset at the end of training\n",
    "                             subwords=False) # Subword embedding learning is not supported by batch_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the sagemaker.session.s3_input objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-30 18:59:58 Starting - Starting the training job...\n",
      "2019-07-30 19:00:10 Starting - Launching requested ML instances......\n",
      "2019-07-30 19:01:11 Starting - Preparing the instances for training...\n",
      "2019-07-30 19:01:49 Downloading - Downloading input data...\n",
      "2019-07-30 19:02:24 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:25 WARNING 140552354150208] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:25 WARNING 140552354150208] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:25 INFO 140552354150208] nvidia-smi took: 0.0252649784088 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:25 INFO 140552354150208] Running single machine CPU BlazingText training using batch_skipgram mode.\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:25 INFO 140552354150208] Processing /opt/ml/input/data/train/transacoesObrasbag.csv . File size: 88 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 33M words\u001b[0m\n",
      "\u001b[31mNumber of words:  83293\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0487  Progress: 2.56%  Million Words/sec: 3.01 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0461  Progress: 7.86%  Million Words/sec: 7.20 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0435  Progress: 13.03%  Million Words/sec: 9.76 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0409  Progress: 18.28%  Million Words/sec: 11.58 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0382  Progress: 23.50%  Million Words/sec: 12.90 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0356  Progress: 28.79%  Million Words/sec: 13.95 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0330  Progress: 34.05%  Million Words/sec: 14.76 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0304  Progress: 39.25%  Million Words/sec: 15.39 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0277  Progress: 44.52%  Million Words/sec: 15.94 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0251  Progress: 49.79%  Million Words/sec: 16.40 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0225  Progress: 55.05%  Million Words/sec: 16.79 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0198  Progress: 60.32%  Million Words/sec: 17.13 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0172  Progress: 65.56%  Million Words/sec: 17.42 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0146  Progress: 70.84%  Million Words/sec: 17.68 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0119  Progress: 76.13%  Million Words/sec: 17.91 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0093  Progress: 81.36%  Million Words/sec: 18.11 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0067  Progress: 86.64%  Million Words/sec: 18.29 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0040  Progress: 91.90%  Million Words/sec: 18.46 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0014  Progress: 97.16%  Million Words/sec: 18.61 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 18.51 #####\u001b[0m\n",
      "\u001b[31mTraining finished!\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 18.51\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 8.92\u001b[0m\n",
      "\u001b[31mEvaluating word embeddings....\u001b[0m\n",
      "\n",
      "2019-07-30 19:02:55 Uploading - Uploading generated training model\u001b[31mVectors read from: /opt/ml/model/vectors.txt \u001b[0m\n",
      "\u001b[31m{\n",
      "    \"EN-WS-353-ALL.txt\": {\n",
      "        \"not_found\": 353, \n",
      "        \"spearmans_rho\": 0.0, \n",
      "        \"total_pairs\": 353\n",
      "    }, \n",
      "    \"EN-WS-353-REL.txt\": {\n",
      "        \"not_found\": 252, \n",
      "        \"spearmans_rho\": 0.0, \n",
      "        \"total_pairs\": 252\n",
      "    }, \n",
      "    \"EN-WS-353-SIM.txt\": {\n",
      "        \"not_found\": 203, \n",
      "        \"spearmans_rho\": 0.0, \n",
      "        \"total_pairs\": 203\n",
      "    }, \n",
      "    \"mean_rho\": 0.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m[07/30/2019 19:02:49 INFO 140552354150208] #mean_rho: 0.0\u001b[0m\n",
      "\n",
      "2019-07-30 19:03:21 Completed - Training job completed\n",
      "Billable seconds: 92\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'vector': [0.05717432498931885, 0.04714442044496536, -0.01673906110227108, 0.02388642355799675, 0.015328881330788136, -0.025969361886382103, -0.11886902898550034, 0.06898565590381622, 0.09335990250110626, -0.02641250565648079, -0.03700384497642517, 0.06896259635686874, -0.031042464077472687, 0.012479080818593502, -0.04531089961528778, 0.10692345350980759, 0.025482043623924255, -0.00020958659297320992, -0.03344803676009178, -0.012593284249305725, 0.07492173463106155, 0.07054547220468521, 0.006326812319457531, -0.0707009807229042, -0.03975049406290054, -0.021173417568206787, 0.06802577525377274, 0.02770390175282955, -0.0837588831782341, 0.05689423158764839, 0.011320154182612896, 0.001598377013579011, 0.04239125922322273, -0.08188647031784058, -0.0483018197119236, -0.026983944699168205, -0.01045744027942419, -0.03256293386220932, -0.0643141120672226, -0.07908020913600922, 0.008036988787353039, 0.004903685301542282, -0.016543073579669, 0.11999335139989853, 0.0431520976126194, -0.0757845789194107, 0.06390488892793655, 0.02554628625512123, 0.01856210269033909, -0.0694928914308548, -0.0942535251379013, -0.0016035661101341248, -0.04872715100646019, 0.10087663680315018, 0.0012129162205383182, -0.12186847627162933, -0.06721506267786026, 0.05659782141447067, 0.07847251743078232, 0.0057765343226492405, 0.10149478912353516, -0.00027857680106535554, 0.16026049852371216, -0.02144278585910797, 0.029506094753742218, -0.01271915901452303, 0.028703244403004646, 0.02068466879427433, 0.0154264522716403, -0.011190361343324184, -0.08478238433599472, -0.01676449179649353, -0.03232826665043831, 0.0044655464589595795, -0.07900465279817581, 0.0065468973480165005, 0.09759850054979324, -0.02096225507557392, -0.0027815448120236397, 0.007631747517734766, 0.03415796533226967, -0.01399595569819212, 0.04245242103934288, -0.019073257222771645, 0.06431745737791061, 0.0019649879541248083, 0.08212702721357346, 0.08463940769433975, -0.014286686666309834, -0.0643497109413147, 0.02657088078558445, -0.005737240891903639, 0.13848474621772766, -0.09796593338251114, -0.00291486456990242, -0.07052157819271088, -0.060851264744997025, 0.04903707653284073, -0.030488139018416405, -0.029141314327716827], 'word': '327797'}, {'vector': [0.032625239342451096, 0.034771598875522614, -0.0027011919301003218, 0.0034726138692349195, 0.008092346601188183, -0.01790240965783596, -0.06738501787185669, 0.042722832411527634, 0.05826079472899437, -0.02086225338280201, -0.014413371682167053, 0.054528843611478806, -0.01663997955620289, 0.007735597435384989, -0.029410822317004204, 0.06018078327178955, 0.014426296576857567, 0.001799493096768856, -0.01810292713344097, -0.0034060077741742134, 0.047195568680763245, 0.0353122279047966, 0.014067022129893303, -0.04226788133382797, -0.020598964765667915, -0.012187558226287365, 0.03992970660328865, 0.023305578157305717, -0.04595198854804039, 0.03553087264299393, -0.0012388862669467926, 0.010605046525597572, 0.023080214858055115, -0.04831699654459953, -0.03270195052027702, -0.01887320727109909, -0.012690452858805656, -0.016154242679476738, -0.050005361437797546, -0.03968159481883049, 0.00857334304600954, 0.003775770543143153, 0.0022948470432311296, 0.06066981330513954, 0.029807506129145622, -0.05184907466173172, 0.039839714765548706, 0.030007150024175644, 0.009259834885597229, -0.042449288070201874, -0.05748560652136803, 0.004358413629233837, -0.03202569857239723, 0.05258438363671303, -0.002025422640144825, -0.07097819447517395, -0.04040010645985603, 0.020710671320557594, 0.04811155050992966, 0.009604273363947868, 0.07284364849328995, 0.0004545032570604235, 0.09542138874530792, -0.0017341884085908532, 0.015691785141825676, -0.01372874528169632, 0.01523089874535799, 0.017215801402926445, -0.0065330034121870995, -0.01562249381095171, -0.05384567752480507, -0.01167464628815651, -0.02782859280705452, 0.007734957616776228, -0.061533838510513306, 0.016839005053043365, 0.060386061668395996, -0.013263284228742123, -0.0018970073433592916, 0.014709481038153172, 0.021906238049268723, -0.008461596444249153, 0.02389821968972683, -0.013145148754119873, 0.03899075463414192, -0.0069710188545286655, 0.04743819683790207, 0.047433506697416306, -0.01239495724439621, -0.038957275450229645, 0.0156692061573267, 0.009160655550658703, 0.08898905664682388, -0.05816826969385147, 0.0007483547669835389, -0.03197076544165611, -0.03126164525747299, 0.02533366158604622, -0.021660326048731804, -0.0071217878721654415], 'word': '16708'}]\n"
     ]
    }
   ],
   "source": [
    "words = [\"327797\", \"16708\"]\n",
    "\n",
    "payload = {\"instances\" : words}\n",
    "\n",
    "response = bt_endpoint.predict(json.dumps(payload))\n",
    "\n",
    "vecs = json.loads(response)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "Let us now download the word vectors learned by our model and visualize them using a t-SNE plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "key = bt_model.model_data[bt_model.model_data.find(\"/\", 5)+1:]\n",
    "s3.Bucket(bucket).download_file(key, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blazingtext_word2vec_text8_2019-07-30  model.tar.gz\n",
      "ClassificadorCDD\t\t       Recomender_amazon_word2Vec.ipynb\n",
      "lost+found\t\t\t       transacoesObrasbag.csv\n",
      "vectors.txt\n",
      "eval.json\n",
      "vectors.bin\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "\n",
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {\n",
    "    1735: 'moreninha',\n",
    "    1522 : \"dom casmurro\",\n",
    "    2172 : \"Quincas Borba: \",\n",
    "    278  : 'Quem Mexeu no Meu Queijo?', \n",
    "    5310: 'Como Fazer Amigos e Influenciar Pessoas',\n",
    "    1117  : 'Fundamentos da metafisica dos costumes ',\n",
    "    9948 : 'Convite a Filosofia:',\n",
    "    277  : 'Grande sertao Veredas'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
